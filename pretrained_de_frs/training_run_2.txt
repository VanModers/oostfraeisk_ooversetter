0% 0/12567 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
{'loss': 0.1541, 'grad_norm': 0.12459298968315125, 'learning_rate': 4.8010662847139335e-05, 'epoch': 0.12}
{'loss': 0.0512, 'grad_norm': 0.1198340654373169, 'learning_rate': 4.602132569427867e-05, 'epoch': 0.24}
{'loss': 0.045, 'grad_norm': 0.12041766941547394, 'learning_rate': 4.4031988541418e-05, 'epoch': 0.36}
{'loss': 0.0413, 'grad_norm': 0.1151723638176918, 'learning_rate': 4.204265138855733e-05, 'epoch': 0.48}
{'loss': 0.0379, 'grad_norm': 0.11487139016389847, 'learning_rate': 4.0053314235696665e-05, 'epoch': 0.6}
{'loss': 0.0364, 'grad_norm': 0.10310293734073639, 'learning_rate': 3.8063977082836e-05, 'epoch': 0.72}
{'loss': 0.0347, 'grad_norm': 0.0987134724855423, 'learning_rate': 3.607463992997534e-05, 'epoch': 0.84}
{'loss': 0.0335, 'grad_norm': 0.11002060770988464, 'learning_rate': 3.408530277711467e-05, 'epoch': 0.95}
 33% 4189/12567 [45:10<1:24:46,  1.65it/s]
  0% 0/32 [00:00<?, ?it/s]
  6% 2/32 [00:01<00:19,  1.57it/s]
  9% 3/32 [00:02<00:26,  1.11it/s]
 12% 4/32 [00:03<00:25,  1.09it/s]
 16% 5/32 [00:04<00:27,  1.03s/it]
 19% 6/32 [00:06<00:31,  1.22s/it]
 22% 7/32 [00:07<00:32,  1.30s/it]
 25% 8/32 [00:08<00:30,  1.27s/it]
 28% 9/32 [00:10<00:31,  1.38s/it]
 31% 10/32 [00:11<00:28,  1.30s/it]
 34% 11/32 [00:12<00:26,  1.28s/it]
 38% 12/32 [00:14<00:25,  1.28s/it]
 41% 13/32 [00:16<00:27,  1.46s/it]
 44% 14/32 [00:17<00:24,  1.39s/it]
 47% 15/32 [00:18<00:21,  1.27s/it]
 50% 16/32 [00:19<00:19,  1.24s/it]
 53% 17/32 [00:21<00:22,  1.53s/it]
 56% 18/32 [00:23<00:21,  1.52s/it]
 59% 19/32 [00:24<00:18,  1.41s/it]
 62% 20/32 [00:25<00:15,  1.30s/it]
 66% 21/32 [00:26<00:14,  1.31s/it]
 69% 22/32 [00:27<00:12,  1.21s/it]
 72% 23/32 [00:28<00:10,  1.18s/it]
 75% 24/32 [00:29<00:09,  1.16s/it]
 78% 25/32 [00:31<00:08,  1.15s/it]
 81% 26/32 [00:32<00:08,  1.37s/it]
 84% 27/32 [00:34<00:06,  1.39s/it]
 88% 28/32 [00:35<00:05,  1.32s/it]
 91% 29/32 [00:36<00:03,  1.26s/it]
 94% 30/32 [00:38<00:02,  1.29s/it]
 97% 31/32 [00:39<00:01,  1.45s/it]
                                          
{'eval_loss': 0.02730119228363037, 'eval_accuracy': 0.3693314955203308, 'eval_runtime': 41.8135, 'eval_samples_per_second': 23.916, 'eval_steps_per_second': 0.765, 'epoch': 1.0}
 33% 4189/12567 [45:52<1:24:46,  1.65it/s]
100% 32/32 [00:40<00:00,  1.18s/it]
                                   /usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[58100]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
{'loss': 0.0305, 'grad_norm': 0.1158011183142662, 'learning_rate': 3.2095965624253995e-05, 'epoch': 1.07}
{'loss': 0.0289, 'grad_norm': 0.10202755779027939, 'learning_rate': 3.0106628471393334e-05, 'epoch': 1.19}
{'loss': 0.0278, 'grad_norm': 0.1023193746805191, 'learning_rate': 2.8117291318532667e-05, 'epoch': 1.31}
{'loss': 0.0275, 'grad_norm': 0.10011395066976547, 'learning_rate': 2.6127954165671996e-05, 'epoch': 1.43}
{'loss': 0.0263, 'grad_norm': 0.09592606127262115, 'learning_rate': 2.4138617012811332e-05, 'epoch': 1.55}
{'loss': 0.0262, 'grad_norm': 0.10178010165691376, 'learning_rate': 2.2149279859950668e-05, 'epoch': 1.67}
{'loss': 0.0255, 'grad_norm': 0.10916529595851898, 'learning_rate': 2.0159942707089997e-05, 'epoch': 1.79}
{'loss': 0.0248, 'grad_norm': 0.13579601049423218, 'learning_rate': 1.817060555422933e-05, 'epoch': 1.91}
 67% 8378/12567 [1:31:02<42:24,  1.65it/s]
  0% 0/32 [00:00<?, ?it/s]
  6% 2/32 [00:01<00:20,  1.48it/s]
  9% 3/32 [00:02<00:25,  1.12it/s]
 12% 4/32 [00:03<00:25,  1.08it/s]
 16% 5/32 [00:04<00:27,  1.01s/it]
 19% 6/32 [00:06<00:32,  1.23s/it]
 22% 7/32 [00:07<00:31,  1.27s/it]
 25% 8/32 [00:08<00:29,  1.24s/it]
 28% 9/32 [00:10<00:30,  1.34s/it]
 31% 10/32 [00:11<00:28,  1.28s/it]
 34% 11/32 [00:12<00:26,  1.27s/it]
 38% 12/32 [00:14<00:25,  1.26s/it]
 41% 13/32 [00:15<00:26,  1.40s/it]
 44% 14/32 [00:17<00:24,  1.35s/it]
 47% 15/32 [00:18<00:21,  1.24s/it]
 50% 16/32 [00:19<00:19,  1.22s/it]
 53% 17/32 [00:21<00:23,  1.54s/it]
 56% 18/32 [00:22<00:20,  1.47s/it]
 59% 19/32 [00:23<00:17,  1.38s/it]
 62% 20/32 [00:25<00:15,  1.28s/it]
 66% 21/32 [00:26<00:14,  1.28s/it]
 69% 22/32 [00:27<00:11,  1.18s/it]
 72% 23/32 [00:28<00:10,  1.16s/it]
 75% 24/32 [00:29<00:09,  1.14s/it]
 78% 25/32 [00:30<00:08,  1.17s/it]
 81% 26/32 [00:32<00:08,  1.40s/it]
 84% 27/32 [00:34<00:07,  1.40s/it]
 88% 28/32 [00:35<00:05,  1.34s/it]
 91% 29/32 [00:36<00:03,  1.32s/it]
 94% 30/32 [00:37<00:02,  1.29s/it]
 97% 31/32 [00:39<00:01,  1.37s/it]
                                          
{'eval_loss': 0.02149301767349243, 'eval_accuracy': 0.41729841488628533, 'eval_runtime': 40.8521, 'eval_samples_per_second': 24.479, 'eval_steps_per_second': 0.783, 'epoch': 2.0}
 67% 8378/12567 [1:31:42<42:24,  1.65it/s]
100% 32/32 [00:39<00:00,  1.13s/it]
{'loss': 0.024, 'grad_norm': 0.09082511067390442, 'learning_rate': 1.6181268401368666e-05, 'epoch': 2.03}
{'loss': 0.0224, 'grad_norm': 0.09884138405323029, 'learning_rate': 1.4191931248507998e-05, 'epoch': 2.15}
{'loss': 0.022, 'grad_norm': 0.1063367947936058, 'learning_rate': 1.220259409564733e-05, 'epoch': 2.27}
{'loss': 0.0217, 'grad_norm': 0.08633153140544891, 'learning_rate': 1.0213256942786665e-05, 'epoch': 2.39}
{'loss': 0.022, 'grad_norm': 0.10585293173789978, 'learning_rate': 8.223919789925997e-06, 'epoch': 2.51}
{'loss': 0.0217, 'grad_norm': 0.11684802174568176, 'learning_rate': 6.23458263706533e-06, 'epoch': 2.63}
{'loss': 0.0214, 'grad_norm': 0.08120410144329071, 'learning_rate': 4.245245484204663e-06, 'epoch': 2.75}
{'loss': 0.0211, 'grad_norm': 0.09325773268938065, 'learning_rate': 2.2559083313439963e-06, 'epoch': 2.86}
{'loss': 0.0208, 'grad_norm': 0.09736299514770508, 'learning_rate': 2.6657117848332936e-07, 'epoch': 2.98}
100% 12567/12567 [2:16:52<00:00,  1.65it/s]
  0% 0/32 [00:00<?, ?it/s]
  6% 2/32 [00:01<00:19,  1.58it/s]
  9% 3/32 [00:02<00:24,  1.16it/s]
 12% 4/32 [00:03<00:25,  1.11it/s]
 16% 5/32 [00:04<00:27,  1.01s/it]
 19% 6/32 [00:06<00:31,  1.19s/it]
 22% 7/32 [00:07<00:31,  1.24s/it]
 25% 8/32 [00:08<00:29,  1.24s/it]
 28% 9/32 [00:10<00:30,  1.34s/it]
 31% 10/32 [00:11<00:28,  1.28s/it]
 34% 11/32 [00:12<00:26,  1.27s/it]
 38% 12/32 [00:13<00:25,  1.27s/it]
 41% 13/32 [00:15<00:26,  1.41s/it]
 44% 14/32 [00:16<00:24,  1.36s/it]
 47% 15/32 [00:18<00:22,  1.34s/it]
 50% 16/32 [00:19<00:20,  1.29s/it]
 53% 17/32 [00:21<00:23,  1.58s/it]
 56% 18/32 [00:22<00:20,  1.50s/it]
 59% 19/32 [00:24<00:17,  1.38s/it]
 62% 20/32 [00:25<00:15,  1.28s/it]
 66% 21/32 [00:26<00:14,  1.28s/it]
 69% 22/32 [00:27<00:11,  1.18s/it]
 72% 23/32 [00:28<00:10,  1.16s/it]
 75% 24/32 [00:29<00:09,  1.14s/it]
 78% 25/32 [00:30<00:08,  1.15s/it]
 81% 26/32 [00:32<00:08,  1.39s/it]
 84% 27/32 [00:34<00:06,  1.39s/it]
 88% 28/32 [00:35<00:05,  1.31s/it]
 91% 29/32 [00:36<00:03,  1.25s/it]
 94% 30/32 [00:37<00:02,  1.23s/it]
 97% 31/32 [00:38<00:01,  1.31s/it]
                                           
{'eval_loss': 0.01973632723093033, 'eval_accuracy': 0.44541695382494834, 'eval_runtime': 40.5224, 'eval_samples_per_second': 24.678, 'eval_steps_per_second': 0.79, 'epoch': 3.0}
100% 12567/12567 [2:17:35<00:00,  1.65it/s]
100% 32/32 [00:39<00:00,  1.09s/it]
{'train_runtime': 8264.0935, 'train_samples_per_second': 48.66, 'train_steps_per_second': 1.521, 'train_loss': 0.03387813806401011, 'epoch': 3.0}
100% 12567/12567 [2:17:37<00:00,  1.52it/s]